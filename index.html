<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="Assessment of Multimodal Large Language Models in Alignment with Human Values">
  <meta name="keywords" content="MLLM, Human-value Alignment">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Assessment of Multimodal Large Language Models in Alignment with Human Values</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/logo.png">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.4/css/all.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">


  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>

  <!-- <nav class="navbar" role="navigation" aria-label="main navigation"> -->
  <!-- <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div> -->
  <!-- <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://github.com/adwardlee">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="./leaderboard.html">
            üèÜLeaderboard
          </a>
          <a class="navbar-item" href="https://github.com/OpenSafetyLab/SALAD-BENCH">
            Github
        </div>
      </div>
    </div>

  </div> -->
  <!-- </nav> -->


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Assessment of Multimodal Large Language Models in Alignment with
              Human Values</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=EDLcoVkAAAAJ&hl=zh-CN">Zhelun
                  Shi</a><sup>1,2,*</sup>,</span>
              <span class="author-block">
                <a href="https://github.com/Puck-U">Zhipin Wang</a><sup>2,*</sup>,</span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=Wnk95ccAAAAJ">Hongxing Fan</a><sup>2,*</sup>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=3SAk3GQAAAAJ&hl=zh-CN&oi=ao">Zaibin
                  Zhang</a><sup>1,3</sup>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=394j5K4AAAAJ&hl=zh-CN&oi=ao">Lijun
                  Li</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://openreview.net/profile?id=~yongting_zhang2">Yongting Zhang</a><sup>1,4</sup>,
              </span>
              <br>
              <span class="author-block">
                <a href="https://scholar.google.com.hk/citations?user=ngPR1dIAAAAJ&hl=zh-CN">Zhenfei
                  Yin</a><sup>1,5</sup>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com.hk/citations?user=_8lB7xcAAAAJ&hl=zh-CN&oi=ao">Lu
                  Sheng</a><sup>2,&dagger;</sup>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=gFtI-8QAAAAJ&hl=zh-CN&oi=ao">Yu Qiao</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://amandajshao.github.io/">Jing Shao</a><sup>1,&dagger;</sup>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>Shanghai Artificial Intelligence Laboratory</span>
              <span class="author-block"><sup>2</sup>Beihang University</span>
              <span class="author-block"><sup>3</sup>Dalian University of Technology</span>
              <span class="author-block"><sup>4</sup>University of Science and Technology of China</span>
              <span class="author-block"><sup>5</sup>The University of Sydney</span>
              <br> <!-- Newline introduced here -->
              <span class="author-block"><sup>*</sup>Equal contribution</span>
              <span class="author-block"><sup>&dagger;</sup>Corresponding author</span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2402.05044" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <!-- Code Link. -->
                <span class="link-block">
                  <a href="https://github.com/OpenGVLab/LAMM" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
                <!-- Leaderboard Link. -->
                <span class="link-block">
                  <a href="./leaderboard.html" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fa-solid fa-trophy"></i>
                    </span>
                    <span>Leaderboard</span>
                  </a>
                </span>
                <!-- Huggingface Learboard Link. -->
                <!-- <span class="link-block">
                <a href="https://huggingface.co/spaces/OpenSafetyLab/Salad-Bench-Leaderboard"
                   class="external-link button is-normal is-rounded is-dark">
                  <span>ü§óLeaderboard</span>
                  </a>
              </span> -->
                <!-- Dataset Link. -->
                <span class="link-block">
                  <a href="https://huggingface.co/datasets/OpenSafetyLab/Salad-Data"
                    class="external-link button is-normal is-rounded is-dark">
                    <span>ü§óData</span>
                  </a>
              </div>

            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <h2 class="title is-3">Overview</h2>
        <div class="content has-text-justified">
          <p>
            The evaluation for MLLMs can be categorized into three ascending levels of alignment.
            <i>Alignment in Semantics (A1)</i> pertains to the model's ability to perceive basic visual information in
            images.
            <i>Alignment in Logic (A2)</i> evaluates the model's capability in integrating its substantial knowledge
            reserves and analytical strengths to process visual context thoughtfully.
            <i>Alignment with Human Values (A3)</i> examines whether the model can mirror human-like engagement in the
            diverse and dynamic visual world meanwhile understand human expectations and preferences.
            The examples for each alignment level are displayed in the upper half. The benchmarks and evaluated
            dimensions are illustrated at each level.
            C<math xmlns="http://www.w3.org/1998/Math/MathML">
              <mrow>
                <msup>
                  <mi>h</mi>
                  <mn>3</mn>
                </msup>
              </mrow>
            </math>Ef
            dataset is the first comprehensive A3 dataset on <b>hhh (helpful, honest, harmless)</b> criteria, and the
            evaluation strategy can be used to evaluate MLLMs on various scenarios across <i>A1-A3</i> spectra.
          </p>
        </div>
        <div style="text-align: center;"> <!-- Centering wrapper for the image -->
          <img src="./static/images/intro_image.png" class="" style="max-width:100%; height:auto;" />
          </img>
        </div>
      </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <h2 class="title is-3">Comparison with other benchmarks</h2>
        <div class="content has-text-justified">
          <p>
            Comparison between various MLLM benchmarks and C<math xmlns="http://www.w3.org/1998/Math/MathML">
              <mrow>
                <msup>
                  <mi>h</mi>
                  <mn>3</mn>
                </msup>
              </mrow>
            </math>Ef. <i>A3*</i> denotes the evaluation of narrow dimensions within the preliminary stage of <i>A3</i>.
            Previous works either only evaluated <i>A1-A2</i> or merely made preliminary explorations at <i>A3</i>,
            assessing a few dimensions.
            C<math xmlns="http://www.w3.org/1998/Math/MathML">
              <mrow>
                <msup>
                  <mi>h</mi>
                  <mn>3</mn>
                </msup>
              </mrow>
            </math>Ef is the first attempt to define and evaluate the capabilities of MLLMs at <i>A3</i>.
          </p>
        </div>
        <img src="./static/images/comparison.png" class="" />
        </img>
      </div>
    </div>
  </section>

  <!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="title is-3">Video</h2>
      <video id="teaser" controls playsinline height="100%">
        <source src="./static/videos/SALAD-Bench.mp4"
                type="video/mp4">
      </video>
    </div>
  </div>
</section> -->

  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Large Language Models (LLMs) aim to serve as versatile assistants aligned with human values, as defined by
              the principles of being <b>helpful</b>, <b>honest</b>, and <b>harmless (hhh)</b>. However, in terms of
              Multimodal Large Language Models (MLLMs), despite their commendable performance in perception and
              reasoning tasks, their alignment with human values remains largely unexplored, given the complexity of
              defining hhh dimensions in the visual world and the difficulty in collecting relevant data that accurately
              mirrors real-world situations. To address this gap, we introduce 
              C<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msup><mi>h</mi><mn>3</mn></msup></mrow></math>Ef
              , a Compreh3ensive Evaluation dataset and strategy for assessing alignment with human
              expectations. C<math xmlns="http://www.w3.org/1998/Math/MathML">
                <mrow>
                  <msup>
                    <mi>h</mi>
                    <mn>3</mn>
                  </msup>
                </mrow>
              </math>Ef dataset contains 1002 human-annotated data samples, covering 12 domains and 46 tasks based on
              the hhh principle. We also present a unified evaluation strategy supporting assessment across various
              scenarios and different perspectives. Based on the evaluation results, we summarize over 10 key findings
              that deepen the understanding of MLLM capabilities, limitations, and the dynamic relationships between
              evaluation levels, guiding future advancements in the field.
            </p>
          </div>
        </div>
      </div>
      <!--/ Abstract. -->
    </div>
  </section>


  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Taxonomy</h2>
          <div class="content has-text-justified">
            <p>
              We integrate the <b>hhh</b> criteria for assessing alignment with human values, and propose three levels of hierarchical dimension. These dimensions focus on their effectiveness in addressing queries and visual content (<b>helpful</b>), transparency about confidence and limitations within visual scenario (<b>honest</b>), and the avoidance of offensive or discriminatory outputs in the visual world (<b>harmless</b>). The taxonomy forms the basis of our comprehensive evaluation, offering a structured methodology to assess MLLMs' alignment with essential human-centric characteristics.
            </p>
            <p>
              The taxonomy emphasizing the <b>hhh</b> criteria, systematically outlines 4/3/5 domains and 22/7/17 tasks for each <b>h</b> respectively. Details of the domains and tasks are illustrated.
            </p>
          </div>
          <div class="content has-text-centered">
            <img src="./static/images/taxonomy.png" class=""
              style="max-width:100%; height:auto;" />
            </img>
          </div>
          <!--/ Re-rendering. -->

        </div>
      </div>
      <!-- <div class="columns is-centered"> -->
        <!-- Visual Effects. -->
        <!-- <div class="column">
        <div class="content">
          <h2 class="title is-3">Safety categories</h2>
          <p>
            Three levels and 65 categories focused on safety issues.
          </p>
          <img src="./static/images/categories.png"
                 class=""
                 alt="ViewNeTI pull figure and sample novel view synthesis results."/>
          </img>
        </div>
      </div> -->
        <!--/ Visual Effects. -->

        <!-- Matting. -->
        <!-- <div class="column">
        <h2 class="title is-3">Data distribution</h2>
        <div class="columns is-centered">
          <div class="column content">
            <p>
              Data source of base set in SALAD-Bench.
            </p>
            <img src="./static/images/data_distribution.png"
                 class=""
                 alt="ViewNeTI pull figure and sample novel view synthesis results."/>
          </img>
          </div>

        </div>
      </div> -->
      <!-- </div> -->
      <!--/ Matting. -->

      <!-- Data collection. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Dataset</h2>
          <div class="content has-text-justified">
            <p>
              Based the defined taxonomy, C<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msup><mi>h</mi><mn>3</mn></msup></mrow></math>Ef dataset is meticulously crafted to closely emulate real-world
              scenarios. We establish several principles to faithfully replicate the conversation between humans
              and MLLMs, incorporating Human-Machine Synergy by utilizing responses from several prominent
              MLLMsduring the data creation process. 
            </p>
            <p>
              To ensure the dataset closely aligns with real-world application scenarios, we adhere to several
              principles. First, we strive for diversity in images, encompassing both single and multiple images,
              with variations in visual content. Images should be sourced from a wide range of scenarios, covering
              a vast array of application contexts. Second, the formulation of questions and answers aims to mirror
              human behavior and preferences as closely as possible while maintaining consistency with the actual
              potential outputs of MLLMs. For <b>harmless</b>, unlike prior works that evaluate with special images or
              prompts diverging from real-world usage scenarios, C<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msup><mi>h</mi><mn>3</mn></msup></mrow></math>Ef dataset
              ensures images and questions closely resemble practical applications, fostering a more authentic representation.
            </p>
          </div>
          <div class="content has-text-centered">
            <img src="./static/images/data_collection_pipeline.png" class=""
              style="max-width:100%; height:auto;" />
            </img>
          </div>
          <div class="content has-text-justified">
            <p>
              Data samples in C<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msup><mi>h</mi><mn>3</mn></msup></mrow></math>Ef dataset are illustrated. Each sample comprises one or more images, accompanied by a meticulously human annotated question and several options. The correct option is indicated in bold.
            </p>
          </div>
          <div class="content has-text-centered">
            <img src="./static/images/sample.png" class=""
              style="max-width:100%; height:auto;" />
            </img>
          </div>
        </div>
      </div>

      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Evaluation Strategy</h2>
          <div class="content has-text-justified">
            <p>
              C<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msup><mi>h</mi><mn>3</mn></msup></mrow></math>Ef
              evaluation strategy comprises three compatible modules,
              i.e., Instruction, Inferencer and Metric, enabling different Recipes (specific selections of
              each module) to facilitate evaluations from different perspectives across various scenarios ranging
              from <i>A1-A3</i> spectra. The right side shows different Recipes for evaluating different dimensions,
              including location (Locat.), QA performance (QAPerf.), in-context learning performance (ICLPerf.), 
              calibration (Calib.) and alignment with human values (Human-value).
            </p>
          </div>
          <div class="content has-text-centered">
            <img src="./static/images/framework.png" class=""
              style="max-width:100%; height:auto;" />
            </img>
          </div>
          <!--/ Re-rendering. -->

        </div>
      </div>

    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <!-- MD-Judge. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Results</h2>
          <div class="content has-text-justified">
            
            <h3 class="subtitle is-5">Main Results across <i>A1-A3</i> Spectra</h3> <!-- Added subtitle for Safety Rate -->
            <p>
              The main results can be found in <a href="./leaderboard.html">C<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msup><mi>h</mi><mn>3</mn></msup></mrow></math>Ef leaderboard.</a>
            </p>
            
            <h3 class="subtitle is-5">Correlations</h3> <!-- Added subtitle for Attack Comparison -->
            <p>
              Pearson correlation matrix within <i>A3</i> and across <i>A1-A3</i>. CDU for Cross-domain understanding; MRC
              for Machine reading comprehension. (b) Pearson correlation matrix across. Cooler colors
              indicate higher correlations.
            <div class="content has-text-centered">
              <img src="./static/images/correlation.png" class=""
                style="max-width:100%; height:auto;" />
            </div>
            </p>
            
            <h3 class="subtitle is-5">Other Results</h3> <!-- Added subtitle for Attack Comparison -->
            <p>
              The left shows experimental results of MMBench with ICE as Instruction under different retriever
              settings. The retriever methodologies employed encompass Random, Fixed, Top-k Text, and Top-k
              Image. The right is the results of Honest and Calibration. Calibration score is calculated by (1-ECE)√ó100%.
            <div class="content has-text-centered">
              <img src="./static/images/iclandcalib.png" class=""
                style="max-width:100%; height:auto;" />
            </div>
            </p>
            
            <h3 class="subtitle is-5">Overall Results on C<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msup><mi>h</mi><mn>3</mn></msup></mrow></math>Ef dataset</h3> <!-- Added subtitle for Attack Comparison -->
            <p>
              We illustrate the accuracy of each MLLM on each task within the C<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msup><mi>h</mi><mn>3</mn></msup></mrow></math>Ef dataset.
            <div class="content has-text-centered">
              <img src="./static/images/overallresults.png" class=""
                style="max-width:100%; height:auto;" />
            </div>
            </p>

          </div>
        </div>
      </div>
      <!--/ MD-Judge. -->
    </div>
  </section>


  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
      TODO
    </code></pre>
    </div>
  </section>


  <footer class="footer" style="height:10px; padding-top: 1rem; padding-bottom: 1em; padding-left: 0px;">
    <div class="container" >
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              The template for this page is designed in <a
                href="https://github.com/adwardlee/view_renderih/">view_renderih</a>.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

</body>

</html>